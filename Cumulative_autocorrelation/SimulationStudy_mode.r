### Simulation study to investigate the performance of the inference appraoch
### incoorparating the cumulative autocorrelation to adjust credibility regions 
### around the posterior mode
### incidence data will be simulated by a seasonal sir model with autocorrelated
### observation residuals generated by either an ARMA, AR or Brownian Bridge process
### investigated inference approaches are MLE assuming indipendent residuals,
### autregressive residuals, and indep. resid. accounting for cumul. autocorr.

### import required function and packages
source(file="effectivesamplesize.R");
source(file="sirseasonal.R");
source(file="datasimulationarma22.R");
source(file="datasimulationBB.R");
source(file="loglikelihood.ind.R");
source(file="loglikelihood.ar.R");

require('deSolve');
require('optimx');
require('mnormt');
require('numDeriv')

### set parameters for data simulation
# initial condition for sir-model (roughly 82,000,000 people) and time horizon
initial <- c(600500,190,399310)*82;  initial <- c(initial,0);
times <- 0:(8*52);
infodata <- list('initial'=initial,'times'=times)

# parameters for the sir-model
par.alpha <-5/3;
par.beta  <-0.3;
# set method for simulation of observation residuals, BrownianBridge or Arma(2,2)
simmethod <- 'BB'  ### either 'BB' or 'arma'
# parameters for simulating the observation residuals (some only relevant for
#  ARMA(2,2) process), sigma for both methods
par.corr  <- c(0.4,0.4);
par.theta <- c(1,-0.6,0);
sigma     <- 0.05;

########### Start of the data simulation and parameter inference ##############
###############################################################################
# set the number of samples K
K <- 1000;
# define storevariables: each method (ar,ind,ess) estimates two parameters 
# (alpha,beta) for all K data samples
ar_estimates  <- matrix(0,nrow=K,ncol=2);
ind_estimates <- matrix(0,nrow=K,ncol=2);
ess_estimates <- matrix(0,nrow=K,ncol=2);
# storevariables for the 2x2 covariance matrices of the K estimates according to 
# (possibly adjusted) Fisher-information (minus the hessian of loglikelihood)
ar_covar   <- array(0,dim=c(K,2,2))               
ind_covar  <- array(0,dim=c(K,2,2))
ess_covar  <- array(0,dim=c(K,2,2))
# storevariables for the results from the probability integral transform (PIT), 
# intuitively the p-value or quantile of the true parameters with respect to the  
# estimated distribution
PV_ar     <- rep(0,K);
PV_ind    <- rep(0,K);
PV_ess    <- rep(0,K);
# also stored are the results from the effective sample size (or cumulative 
# autocorrelation) estimates, i.e. K time the order of the fitted ARMA process 
# and the CA itself
effss <- matrix(0, nrow=K, ncol=3);

### start of the simulation loop
for (i in 1:K){

# simulation of data using the functions 'datasim.arma22' or 'datasim.BB', both
# functions utilize the model parameters and parameters for the correlation structure
# both functions return the logs of the weekly number of cases for eight years 
if (simmethod=='arma'){
dat <- datasim.arma22(par.alpha=par.alpha,par.beta=par.beta,par.corr=par.corr,
                      par.theta=par.theta,sigma=sigma,infolist=infodata)
}
if (simmethod=='BB'){
dat <- datasim.BB(par.alpha=par.alpha,par.beta=par.beta,sigma=sigma,    
                  infolist=infodata)
}

### Estimation of parameters alpha and beta based on the simulated data (assuming
### the initial condition of the SIR-model is known) ###########################
# define available information
infoest <- list('initial'=initial,'times'=times,'data'=dat)

### first: Max-Likelihood-Estimation assuming AR-Residuals #####################
# for simplicity the optimisation procedure (Nelder-Mead) is initialized at the true
# value
para <- c('par.alpha'=par.alpha,'par.beta'=par.beta,'par.corr'=par.corr[1],'par.sd'=sigma);
# optpara_ar stores the MLE-estimated for the four parameters (alpha,beta,order-1-correlation,sigma)
optpara_ar <- optim(para,loglikelihood.ar,method='Nelder-Mead',infolist=infoest,
                    control=list(maxit=10000,'trace'=0,reltol=1e-8),hessian=FALSE);
# covar_ar estimated the covariance (uncertainty) of the parameters based on 
# the observed Fisher information
covar_ar <- solve(hessian(func=loglikelihood.ar,x=optpara_ar[[1]],
                  method="Richardson",method.args=list(r=2),infolist=infoest));
# store estimates and corresponding uncertainty of the parameters (alpha,beta),
# which is just the submatrix of the whole covariance matrix
ar_estimates[i,] <- optpara_ar[[1]][1:2];
ar_covar[i,,]     <- covar_ar[1:2,1:2];
# compute the effective distance d of the true vector to the estimated vector 
# subject to the estimated covariance matrix
d<-(c(par.alpha,par.beta)-optpara_ar[[1]][1:2])%*%solve(covar_ar[1:2,1:2])%*%(c(par.alpha,par.beta)-optpara_ar[[1]][1:2])
# assuming the true value is indeed from the estimated distribution (normal with 
# estimated mean and covariance), then d is Chi^2 with 2 degrees of freedom and
# thus the quantile is given by:
PV_ar[i] <- 1-exp(-d/2);


### Second: MLE assuming independent residual ##################################
# same steps as above, but with a different loglikelihood function
para <- c('par.alpha'=par.alpha,'par.beta'=par.beta,'par.sd'=sigma)
optpara_ind <- optim(para,loglikelihood.ind,method='Nelder-Mead',infolist=infoest,
                     control=list(maxit=10000,'trace'=0,reltol=1e-8),hessian=FALSE);
covar_ind <- solve(hessian(func=loglikelihood.ind,x=optpara_ind[[1]],
                   method="Richardson",method.args=list(r=2),infolist=infoest));
ind_estimates[i,] <- optpara_ind[[1]][1:2];
ind_covar[i,,]  <- covar_ind[1:2,1:2];
d<-(c(par.alpha,par.beta)-optpara_ind[[1]][1:2])%*%solve(covar_ind[1:2,1:2])%*%(c(par.alpha,par.beta)-optpara_ind[[1]][1:2])
PV_ind[i] <- 1-exp(-d/2);

### Third: CA-Method: accounting for autocorrelation ###########################
# point estimates are the same when assuming indipendent residuals, thus one can
# immediately compute the log observation residuals from the expected observations
# according to the model
# sol is the model output for the MLE assuming indendent residuals
sol <- rk(initial,times,sir,list('par.alpha'=optpara_ind[[1]][1],'par.beta'=optpara_ind[[1]][2]),method='rk4');
sol <- sol[,2:5];
# the last column of sol contains the number of so far occured cases
# log.obsmean computes the log of the weekly cases numbers according to the model 
log.obsmean <- log(sol[2:length(times),4]-sol[(1:length(times)-1),4]); 
# x is the time series of the residuals
x<- dat-log.obsmean;

# the effective sample size (ESS) is comuted based on x
effsamplesize <- effectivesamplesize(x);
effss[i,] <- c(effsamplesize[[1]],effsamplesize[[2]],effsamplesize[[3]])
# the estimated uncertainty is the adjusted based on the ESS
# in rare cases ESS is negative, then it will not be accounted for
if (effsamplesize[['ESS']]>0){covar_ess <- covar_ind/effsamplesize[['ESS']];
}else{covar_ess <- covar_ind;}
# store results and compute the PIT
ess_estimates[i,] <- optpara_ind[[1]][1:2];
ess_covar[i,,]     <- covar_ess[1:2,1:2];
d<-(c(par.alpha,par.beta)-optpara_ind[[1]][1:2])%*%solve(covar_ess[1:2,1:2])%*%(c(par.alpha,par.beta)-optpara_ind[[1]][1:2])
PV_ess[i] <- 1-exp(-d/2);

### plotting of the PIT results, a uniform distribution indicates a good assessment
### of the estimated confidence regions, i.e. covariance matrix
layout(matrix(1:3,nrow=1))
hist(PV_ar[1:i],breaks=(0:10)/10,main='Normal Approximation: AR Residuals',xlab='PIT')
hist(PV_ind[1:i],breaks=(0:10)/10,main='Normal Approximation: Ind Residuals',xlab='PIT')
hist(PV_ess[1:i],breaks=(0:10)/10,main='Normal Approximation: CA Method',xlab='PIT')
}

### save results
save(par.corr,par.theta,sigma,ar_estimates,ind_estimates,ess_estimates,ar_covar,
     ind_covar,ess_covar,PV_ar,PV_ind,PV_ess,file=paste('mode_simresults_',simmethod,'.Rdata'));





 